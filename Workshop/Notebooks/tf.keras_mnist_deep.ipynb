{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tf.keras_mnist_deep.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["BzzDMryNsGZV","i7dx_4V38Qfm","omdwB1KRswta","VlaCFqlDy4dC","2ScoLr1J2ZcC","fo2FUMVL4bK6"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BzzDMryNsGZV","colab_type":"text"},"source":["#Tutorial: Deep Neural Networks\n","In this tutorial we will train a deep NN on the MNIST data set using Rectified Linear Unit (ReLU) activation in the hidden units and early stopping.  We will assume a 'sequential' model, densely connected layers and the RMSprop optimizer which are described in detail in the Tensorflow Keras documentation (see https://www.tensorflow.org/api_docs/python/tf/keras)"]},{"cell_type":"markdown","metadata":{"id":"i7dx_4V38Qfm","colab_type":"text"},"source":["#Step 0 - Setup the learning environment\n","\n","To begin, we import the Keras modules and functions that we will use, plus some plotting tools and then we set up the high-level learning parameters.  "]},{"cell_type":"code","metadata":{"id":"dOLnEljVzAGY","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.datasets import mnist\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","np.random.seed(1)\n","\n","train_size = 30000\n","batch_size = 128\n","num_classes = 10\n","epochs = 10\n","\n","print(tf.VERSION)\n","print(tf.keras.__version__)\n","\n","print('Environment setup complete!')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"omdwB1KRswta","colab_type":"text"},"source":["#Step 1 - Load and prepare the data\n","Note that we have 60k records for training (10k of which will be set aside for validation or 'tuning', the remaining 50k go toward the 'train' partition) and an additional 10k (*unseen*) test records.  The 784 dimension indicates flattening of the images that come as squares by default in the MNIST set. The class vectors are converted to 'categorical' labels for compatibility with Keras. The to_categorical function simply converts a class vector (integers) to binary class matrix."]},{"cell_type":"code","metadata":{"id":"lx_OY0a5xaXx","colab_type":"code","colab":{}},"source":["math.sqrt(784)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uRrZS2S-u7gB","colab_type":"code","colab":{}},"source":["# Load the train and test sets \n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","x_train = x_train.reshape(60000, 784)\n","x_tune = x_train[50000:60000]\n","x_train = x_train[0:train_size]\n","x_test = x_test.reshape(10000, 784)\n","x_train = x_train.astype('float32')\n","x_tune = x_tune.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_tune /= 255\n","x_test /= 255\n","\n","# Print how many of each\n","print(x_train.shape[0], 'train samples')\n","print(x_tune.shape[0], 'tune samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# Convert class vectors to binary class matrices\n","y_tune = tf.keras.utils.to_categorical(y_train[50000:60000], num_classes)\n","y_train = tf.keras.utils.to_categorical(y_train[0:train_size], num_classes)\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VlaCFqlDy4dC","colab_type":"text"},"source":["#Step 2 - Architecture defintion\n","This model will consist of five layers (in, out and three hidden layers).  Note that we will assume a dropout rate of 0.2 meaning that there is a 20% chance that any given unit will become unavailable at each training epoch.  \n","\n","RMSprop:\tOptimizer keeps\ta\tmoving\taverage\tof\tthe\tsquared\tgradient\tfor\teach\tweight.[http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf] \n","\n","A lot more about the sequential model API that we are using here can be found at: [https://keras.io/models/sequential/]"]},{"cell_type":"code","metadata":{"id":"FuDOR5d1zpw_","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential()\n","\n","# Model consists of five layers as below\n","# Note: we do not explicitly define an input layer\n","model.add(layers.Dense(512, activation='relu', input_shape=(784,)))\n","model.add(layers.Dropout(0.2))\n","model.add(layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n","model.add(layers.Dropout(0.2))\n","model.add(layers.Dense(num_classes, activation='softmax'))\n","\n","# Print a quick summary of the model to verify our work\n","model.summary()\n","\n","# Compile using some 'typical' parameters for loss, optimizer and error metric\n","model.compile(loss=tf.keras.losses.categorical_crossentropy,\n","              optimizer=tf.keras.optimizers.RMSprop(),\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ScoLr1J2ZcC","colab_type":"text"},"source":["#Step 3 - Train the model \n","The model API does all of the heavy lifting here, simply calling the fit and evalutate functions returns everything that we need to know about the model's results on train and test (NB: The tune partition being passed as validation data that will be used for early stopping). "]},{"cell_type":"code","metadata":{"id":"1QA0gW-u2lCh","colab_type":"code","colab":{}},"source":["# Configure early stopping using validation accuracy from tune partition  \n","stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_acc', \n","                                           patience=3, \n","                                           verbose=1, \n","                                           restore_best_weights=True)\n","\n","# The history structure keeps tabs on what happened during the session\n","history = model.fit(x_train, y_train,\n","                    batch_size=batch_size,\n","                    epochs=epochs,\n","                    verbose=1,\n","                    validation_data=(x_tune, y_tune),\n","                    callbacks=[stop_early],\n","                    shuffle=True)\n","\n","score = model.evaluate(x_test, y_test, verbose=0)\n","\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])\n","\n","predictions = model.predict(x_test, verbose=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fo2FUMVL4bK6","colab_type":"text"},"source":["# Step 4 - Plot the results\n","Simply plotting the actual test image along with the what the model predicted (pred) as a value represented in the image.  We are plotting 5 results but setting r to something else will print more or less results.   "]},{"cell_type":"code","metadata":{"id":"dWtdGWid4k7p","colab_type":"code","colab":{}},"source":["r = 10\n","\n","for i in range(r):\n","    subplt = plt.subplot(int(i / r) + 1, r, i + 1)\n","    # no sense in showing labels if they don't match the letter\n","    hot_index = np.argmax(predictions[i])\n","    subplt.set_title('P:{0}'.format(hot_index))\n","    subplt.axis('off')\n","    letter = x_test[i]\n","    subplt.matshow(np.reshape(letter, [28, 28]))\n","    plt.draw()\n","    \n","plt.show()"],"execution_count":0,"outputs":[]}]}