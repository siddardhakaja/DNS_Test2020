{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"perceptron.ipynb","version":"0.3.2","provenance":[{"file_id":"1oro1SXXYeA6sZvt6Hvy4tFpJj9PwN-33","timestamp":1552860607675}],"collapsed_sections":["zRg6PpX8oFJX","iS-naelbqTyc","b60z13SJa4oe","Mo4ia8Z3cFkq","MOj06IqYrIl3"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zRg6PpX8oFJX","colab_type":"text"},"source":["#Step 0 - Setup the learning environment\n","\n","To begin, we import some library modules and functions that we will use.   \n","\n","Numpy is a collection of math functions including various matrix operations - see http://www.numpy.org/.   Matplotlib is a 2D plotting library  - see https://matplotlib.org/.\n"]},{"cell_type":"code","metadata":{"id":"csX9Mdqwg7mb","colab_type":"code","colab":{}},"source":["\"\"\"\n","perceptron.py\n","\n","Backpropagation tutorial using a basic one-layer perceptron, without\n","the aid of additional NN libraries. \n","This one-layer, one node ANN has only an output layer but can learn \n","OR and AND functions.\n","\n","The program uses  Numpy for faster matrix operations and \n","Matplotlib for plotting the training error per epoch\n","\n","\"\"\"\n","from __future__ import print_function\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(\"The enviriment is ready.\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iS-naelbqTyc","colab_type":"text"},"source":["#Step 1 -  Create the training data.\n","The data can be examples of any simple two variable boolean logic function such as OR, AND, or XOR."]},{"cell_type":"code","metadata":{"id":"CHpUkcreqBCx","colab_type":"code","colab":{}},"source":["# Define the function inputs values: bias row, x1 row, x2 row\n","#data_in = np.array([(1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)])\n","#data_in = data_in.T\n","data_in = np.array([(1, 1, 1, 1), (0, 1, 0, 1), (0, 0, 1, 1)])\n","\n","# Define the function through the sequence of target output values\n","target = np.array([0, 1, 1, 1])  # OR\n","#target = np.array([0, 0, 0, 1])  # AND\n","#target = np.array([0, 1, 1, 0])  # XOR\n","\n","print(data_in.T)\n","print(target)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b60z13SJa4oe","colab_type":"text"},"source":["#Step 2 - Configure the neural network architecture \n","This simple network has no hidden nodes, just three weights associated with the output node. "]},{"cell_type":"code","metadata":{"id":"qRPPntzPa2h1","colab_type":"code","colab":{}},"source":["# Set the learing parameters\n","learning_rate = 0.1\n","num_epochs = 200\n","\n","# Set the input and output dimensions\n","size_in = data_in.shape[0]\n","size_out = target.ndim\n","\n","# Set the seed value of the random number generator\n","random_seed = 2\n","np.random.seed(random_seed)\n","\n","# Initialize the weights of the network\n","weight_out = np.random.rand(size_out, size_in) - 0.5\n","\n","# Initialize a vector to store the train errors for each epoch\n","error_log = np.zeros(num_epochs)\n","\n","print(weight_out)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mo4ia8Z3cFkq","colab_type":"text"},"source":["#Step 3 - Fit the data to the model\n","Use either the original Perceptron node with a step activation function \n","Or a more modern Perceptron node with a sigmoid activation function.\n","\n","The model uses the mean squared error (MSE) loss function, which works well with the sigmoid activation output nodes. \n","It uses the gradient descent weight update algorithm.\n"]},{"cell_type":"code","metadata":{"id":"RLytVTNRcGgp","colab_type":"code","colab":{}},"source":["\n","# This code uses a step function for the activation function \n","for i in range(0, num_epochs):\n","    predicted_out = np.dot(weight_out, data_in) > 0\n","    weight_out = weight_out + learning_rate * np.dot((target - predicted_out), data_in.T)\n","    error_log[i] = 0.5 * ((target - predicted_out) ** 2).mean(axis=None)\n","    if (i % 20) == 0:\n","        print(\"Iter: %d, OR MSE: %8.7f\" % (i, error_log[i]))        \n","\n","\n","# This code uses a sigmoid function for the activation function \n","\n","# for i in range(0, num_epochs):\n","#     predicted_out = 1 / (1 + np.exp(np.dot(-weight_out, data_in)))\n","#     deriv_out = (predicted_out * (1 - predicted_out)) * (target - predicted_out)\n","#     deriv_weight_out = learning_rate * np.dot(data_in, deriv_out.T).T\n","#     weight_out = weight_out + deriv_weight_out\n","#     error_log[i] = 0.5 * ((target - predicted_out) ** 2).mean(axis=None)\n","#     if (i % 20) == 0:\n","#         print(\"Iter: %d, MSE: %8.7f\" % (i, error_log[i]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MOj06IqYrIl3","colab_type":"text"},"source":["#Step 4 - Evaluate the model on the training set and print the results.\n","And then plot the training error for each epoch through the data."]},{"cell_type":"code","metadata":{"id":"F7dW18Unr7pM","colab_type":"code","colab":{}},"source":["# Print the traget values and the networks predictions\n","print(\"Target outputs:\", target)\n","print(\"Predicted outputs:\", predicted_out)\n","\n","# Set up the plot of the training error by epoch\n","plt.figure(1)\n","plt.xlabel('Training Epochs')\n","plt.ylabel('Mean Square Error')\n","plt.title('Simple Perceptron')\n","plt.plot(error_log)\n","plt.draw()\n","\n","plt.show()  # keeping the plots alive until you close them"],"execution_count":0,"outputs":[]}]}