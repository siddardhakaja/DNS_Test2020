{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mlp_bp_ann.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["1ZDoeuUgq0t1","rO-ApF4yzD4A","4Di75BregPPS","AAPuFJx1gsO0","EQLBv6nerI_I"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1ZDoeuUgq0t1","colab_type":"text"},"source":["#Step 0 - Setup the learning environment\n","\n","To begin, we import some library modules and functions that we will use.   \n","\n","Numpy is a collection of math functions including various matrix operations - see http://www.numpy.org/.   Matplotlib is a 2D plotting library  - see https://matplotlib.org/."]},{"cell_type":"code","metadata":{"id":"MucjMNbIhdgD","colab_type":"code","colab":{}},"source":["\"\"\"\n","mlp_bp_ann.py\n","\n","Backpropagation tutorial using a two layer ANN, without\n","the aid of additional NN libraries. \n","This two-layer ANN has one layer of hidden nodes that allows the model to develop\n","non-linear functions\n","\n","The program uses  Numpy for faster matrix operations and \n","Matplotlib for plotting the training error per epoch\n","\n","\"\"\"\n","\n","from __future__ import print_function\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(\"The enviriment is ready.\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rO-ApF4yzD4A","colab_type":"text"},"source":["#Step 1 - Create the training data.\n","The data can be examples of any simple two variable boolean logic function such as OR, AND, or XOR.\n","\n"]},{"cell_type":"code","metadata":{"id":"I5b66eR-rHHN","colab_type":"code","colab":{}},"source":["# Define the function inputs and target output values: bias row, x1 row, x2 row\n","data_in = np.array([(1, 1, 1, 1), (0, 1, 0, 1), (0, 0, 1, 1)])\n","#target = np.array([0, 1, 1, 1])  # OR\n","#target = np.array([0, 0, 0, 1])  # AND\n","target = np.array([0, 1, 1, 0])  # XOR\n","\n","print(data_in.T)\n","print(target)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Di75BregPPS","colab_type":"text"},"source":["#Step 2 - Configure the neural network architecture \n","Define a back-propagation neural network with one hidden layer of two nodes and one output node.  \n"]},{"cell_type":"code","metadata":{"id":"N2Kea7YlfRMA","colab_type":"code","colab":{}},"source":["# Set the learing parameters\n","learning_rate = 0.1\n","momemtum = 0.0\n","num_epochs = 20000\n","\n","# Set the dimensions of the input, hidden, and output layers\n","size_in = 2\n","size_hidden = 2  # Can be anything - larger will take longer\n","size_out = target.ndim\n","\n","# Set the seed value of the random number generator\n","random_seed = 2\n","np.random.seed(random_seed)\n","\n","# Initialize the network weights, and place to store previous epoch weights\n","weight_out = np.random.rand(size_out, size_hidden + 1) - 0.5\n","weight_hidden = np.random.rand(size_hidden, size_in + 1) - 0.5\n","\n","weight_hidden_prev = np.zeros(weight_hidden.shape)\n","weight_out_prev = np.zeros(weight_out.shape)\n","\n","# Initialize a vector to store the train errors for each epoch\n","error_log = np.zeros([num_epochs])\n","\n","print(\"Outout node initial weights:\")\n","print(weight_out)\n","print(\"Hidden node initial weights:\")\n","print(weight_hidden)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AAPuFJx1gsO0","colab_type":"text"},"source":["#Step 3 - Fit the data to the model\n","The model uses the mean squared error (MSE) loss function, which works well with the sigmoid activation output nodes. \n","It uses the gradient descent weight update algorithm.\n"]},{"cell_type":"code","metadata":{"id":"aUsIPYqke-Lf","colab_type":"code","colab":{}},"source":["for i in range(0, num_epochs):\n","\n","    # Compute the predicted output for the hidden layer, then adding bias\n","    predicted_h = 1 / (1 + np.exp(np.dot(-weight_hidden, data_in)))\n","    predicted_h = np.concatenate((np.ones([1, data_in.shape[1]]), predicted_h), axis=0)\n","\n","    # Compute the output of network\n","    predicted_out = 1 / (1 + np.exp(np.dot(-weight_out, predicted_h)))\n","\n","    # Compute the derivatives for the weight updates\n","    deriv_out = (predicted_out * (1 - predicted_out)) * (target - predicted_out)\n","    deriv_h = (predicted_h * (1 - predicted_h)) * (weight_out.T * deriv_out)\n","\n","    # Compute the update to the input to hidden node weights\n","    deriv_weight_h = learning_rate * np.dot(\n","        data_in, deriv_h[1:].T).T + momemtum * weight_hidden_prev\n","    weight_hidden = weight_hidden + deriv_weight_h\n","    weight_hidden_prev = deriv_weight_h\n","\n","    # Compute the update to the hidden to output node weights\n","    deriv_weight_out = learning_rate * np.dot(\n","        predicted_h, deriv_out.T).T + momemtum * weight_out_prev\n","    weight_out = weight_out + deriv_weight_out\n","    weight_out_prev = deriv_weight_out\n","\n","    # Compute the error (loss) of the network for this epoch \n","    predicted_out_err = 1 / (1 + np.exp(np.dot(-weight_out, np.concatenate(\n","        (np.ones([1, 4]), (1 / (1 + np.exp(np.dot(-weight_hidden, data_in)))))))))\n","    error_log[i] = 0.5 * ((predicted_out_err - target) ** 2).mean(axis=None)\n","    if (i % 50) == 0:\n","        # print('XOR bias momentum MSE: {0}'.format(error_log[i]))\n","        print(\"Iter: %d, MSE: %8.7f\" % (i, error_log[i]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQLBv6nerI_I","colab_type":"text"},"source":["#Step 4 - Evaluate the model on the training set and print the results.\n","And then plot the training error for each epoch through the data."]},{"cell_type":"code","metadata":{"id":"2O-8W7vRrL6p","colab_type":"code","colab":{}},"source":["# Print the traget values and the networks predictions\n","print(\"Target outputs:\", target)\n","print(\"Predicted outputs:\", predicted_out)\n","\n","# Set up the plot of the training error by epoch\n","plt.figure(4)\n","plt.xlabel('Training Epochs')\n","plt.ylabel('Mean Square Error')\n","plt.title('MLP with two hidden nodes with bias and momentum for XOR')\n","plt.plot(error_log)\n","plt.draw()\n","\n","plt.show()  # keeping the plots alive until you close them"],"execution_count":0,"outputs":[]}]}